Analytical queries : Nvidia GPU vs. SPARC Server

The TPC Benchmark (TPC-H) is a decision support benchmark. It consists of a suite of business oriented queries
that can be used to measure the processing power of a system.
Lets compare top results achieved using Sun Fire with results obtained using a GPU.
We will use a scale of 1000 which means that the size of database is around 1000GB and the largest table 
consists of 6 billion records.

System configuration 1 : [1]
   SPARC T4-4 Server with
   4 SPARC T4 3GHz Processors, 32 cores, 256 threads
   512 GB memory
   4 Sun Storage F5100 Flash Arrays w/ 80 24GB FMODs each
Software : Oracle Database 11g Release 2 Enterprise Edition
Total cost : $925,000 
   
System configuration 2 : 
   Pentium G620 2.6GHz 2 core CPU
   16 GB memory
   1 x 120GB internal SSD
   1 Nvidia C2050 GPU
Software : Alenka GPU database 
Total cost : $2350 

SQL for test query looks like this :

  select l_returnflag, l_linestatus, sum(l_quantity) as sum_qty, sum(l_extendedprice) as sum_base_price,
         sum(l_extendedprice*(1-l_discount)) as sum_disc_price,sum(l_extendedprice*(1-l_discount)*(1+l_tax)) as sum_charge,
         avg(l_quantity) as avg_qty, avg(l_extendedprice) as avg_price, avg(l_discount) as avg_disc,count(*) as count_order
  from lineitem
  where l_shipdate <= date '1998-12-01' - interval '[DELTA]' day (3)
  group by l_returnflag, l_linestatus
  order by l_returnflag, l_linestatus;


Results in seconds of query 1 :

    SPARK      GPU
Q1   189s      173s

Alenka excels at scanning large tables and when using a modern GPU with large amount of memory
it is capable of outperforming large servers that use a state of the art database software.

So what makes it fast ? 

Original lineitem table takes 740 GB of data. Lets see how we can get it down to a manageable size.

1.Alenka is a columnar database. Which means that it needs to read from the disk only the columns that are actually used in a query.
  Different columns are stored in different files. Column that we use take only 240 GB of data.
2.Compression. Original 240 GB of column data are compressed to 45 GB using frame-of-reference and dictionary compression.
  Decompression, like compression is done in GPU and achieves the speed tens of gigabytes per second.
  Alenka keeps data in memory compressed. No need to buy several hundred gigabytes of memory just to load a 1000GB database.
3.Vector processing. The operations can be performed on all data at once unlike traditional databases such as MS SQL Server and Oracle.
4.No need to create and maintain indexes. Alenka uses Netezza-style zone maps for data segments. 

Technical details needed to repeat results :
use dbgen to generate TPCH data with needed scale

compile alenka from source
( nvcc -arch sm_20 -lcuda ./bison.cu -o ./alenka )

run alenka scripts to create data files (load_lineitem.sql)
using segment size of 10000000 (option -l 10000000)

run queries q1.sql

see the results in mytest.txt file.

Alenka have been tested on 64bit Windows7 and Linux.

[1] http://www.tpc.org/tpch/results/tpch_result_detail.asp?id=111092601

